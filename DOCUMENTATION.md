# B√ÅO C√ÅO K·ª∏ THU·∫¨T: ZERO-SHOT DEPTH-AWARE IMAGE EDITING
**Ch·ªânh s·ª≠a ·∫£nh nh·∫≠n bi·∫øt ƒë·ªô s√¢u kh√¥ng c·∫ßn hu·∫•n luy·ªán l·∫°i**

Ngu·ªìn t√†i li·ªáu: Parihar et al., "Zero-Shot Depth-Aware Image Editing with Diffusion Models", ICCV 2025.

---

## 0. B·∫¢NG THU·∫¨T NG·ªÆ & ƒê·ªäNH NGHƒ®A

| Thu·∫≠t ng·ªØ | Gi·∫£i th√≠ch chi ti·∫øt |
|-----------|-------------------|
| **Zero-Shot** | Kh·∫£ nƒÉng th·ª±c hi·ªán t√°c v·ª• m·ªõi m√† kh√¥ng c·∫ßn hu·∫•n luy·ªán l·∫°i. AI c√≥ th·ªÉ d√πng ngay tr√™n d·ªØ li·ªáu ch∆∞a t·ª´ng th·∫•y m√† v·∫´n ho·∫°t ƒë·ªông t·ªët. |
| **Diffusion Models** | M√¥ h√¨nh khu·∫øch t√°n - ph∆∞∆°ng ph√°p sinh ·∫£nh b·∫±ng c√°ch h·ªçc kh·ª≠ nhi·ªÖu. B·∫Øt ƒë·∫ßu t·ª´ ·∫£nh nhi·ªÖu ho√†n to√†n, d·∫ßn d·∫ßn kh√¥i ph·ª•c l·∫°i ·∫£nh r√µ n√©t. V√≠ d·ª•: Stable Diffusion. |
| **Depth-Aware** | Nh·∫≠n th·ª©c ƒë·ªô s√¢u - kh·∫£ nƒÉng hi·ªÉu kh√¥ng gian 3D (tr·ª•c Z). M√°y bi·∫øt v·∫≠t n√†o ·ªü g·∫ßn, v·∫≠t n√†o ·ªü xa. |
| **Occlusion** | S·ª± che khu·∫•t - hi·ªán t∆∞·ª£ng v·∫≠t ·ªü g·∫ßn che khu·∫•t v·∫≠t ·ªü xa. X·ª≠ l√Ω ƒë√∫ng occlusion l√† y·∫øu t·ªë s·ªëng c√≤n ƒë·ªÉ ·∫£nh gh√©p t·ª± nhi√™n. |
| **Inpainting** | V·∫Ω b√π/ƒêi·ªÅn khuy·∫øt - k·ªπ thu·∫≠t d√πng AI t·ª± ƒë·ªông v·∫Ω l·∫°i ph·∫ßn h√¨nh ·∫£nh b·ªã m·∫•t ho·∫∑c b·ªã che khu·∫•t. |
| **Latent Space** | Kh√¥ng gian ti·ªÅm ·∫©n - d·ªØ li·ªáu ·∫£nh ƒë∆∞·ª£c n√©n th√†nh vector ƒë·∫∑c tr∆∞ng ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n. |
| **Self-Attention** | C∆° ch·∫ø t·ª± ch√∫ √Ω - c√°ch AI x√°c ƒë·ªãnh m·ªëi quan h·ªá gi·ªØa c√°c ƒëi·ªÉm ·∫£nh. |
| **Feature Injection** | Ti√™m ƒë·∫∑c tr∆∞ng - can thi·ªáp v√†o m√¥ h√¨nh ƒë·ªÉ √©p n√≥ tu√¢n theo c·∫•u tr√∫c mong mu·ªën. |
| **Data Parallelism** | Song song d·ªØ li·ªáu - chia d·ªØ li·ªáu th√†nh nhi·ªÅu ph·∫ßn ƒë·ªÉ x·ª≠ l√Ω tr√™n nhi·ªÅu GPU c√πng l√∫c. |

---

## 0.1 CHI TI·∫æT: LATENT SPACE (Kh√¥ng gian ti·ªÅm ·∫©n)

### Kh√°i ni·ªám c∆° b·∫£n

H√¨nh ·∫£nh b√¨nh th∆∞·ªùng ·ªü **pixel space:**
- ·∫¢nh 512√ó512√ó3 = **786,432 gi√° tr·ªã pixel** (R, G, B)
- M·ªói gi√° tr·ªã 0-255
- Kh√≥ h·ªçc v√¨ d·ªØ li·ªáu qu√° l·ªõn

**Latent Space** = kh√¥ng gian n√©n l·∫°i:
- ·∫¢nh 512√ó512 ‚Üí n√©n th√†nh **64√ó64√ó4 = 16,384 gi√° tr·ªã** (roughly 48 l·∫ßn nh·ªè h∆°n)
- Gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng, b·ªè chi ti·∫øt kh√¥ng c·∫ßn
- M√¥ h√¨nh h·ªçc nhanh, ti√™u t·ªën memory √≠t

### C√°ch ho·∫°t ƒë·ªông

**VAE Encoder (M√£ h√≥a):**
```
·∫¢nh 512√ó512√ó3 (786K values)
    ‚Üì
CNN layers (conv, pooling, residual blocks)
    ‚Üì
Bottleneck layer ‚Üí gaussian distribution
    ‚Üì
Latent vector 64√ó64√ó4 (16K values)
```

**VAE Decoder (Gi·∫£i m√£):**
```
Latent vector 64√ó64√ó4
    ‚Üì
Transposed CNN (upsampling, conv)
    ‚Üì
·∫¢nh 512√ó512√ó3
```

### T·∫°i sao quan tr·ªçng?

1. **T·ªëc ƒë·ªô:** Diffusion model t·∫°o ti·∫øng ·ªìn trong latent space (nhanh 8x)
2. **Memory:** L∆∞u 16K gi√° tr·ªã thay v√¨ 786K (ti·∫øt ki·ªám VRAM)
3. **Ch·∫•t l∆∞·ª£ng:** M√¥ h√¨nh h·ªçc ƒë·∫∑c tr∆∞ng thay v√¨ chi ti·∫øt pixel v√¥ nghƒ©a
4. **Linh ho·∫°t:** C√≥ th·ªÉ d√πng c√πng m·ªôt latent space cho nhi·ªÅu task

**V√≠ d·ª• th·ª±c t·∫ø:**
- Stable Diffusion: s·ª≠ d·ª•ng VAE ƒë·ªÉ l√†m vi·ªác ·ªü latent space
- Khi·∫øn m√¥ h√¨nh ch·ªâ c·∫ßn ~1.5 t·ª∑ tham s·ªë thay v√¨ 10+ t·ª∑

---

## 0.2 CHI TI·∫æT: SELF-ATTENTION (C∆° ch·∫ø t·ª± ch√∫ √Ω)

### Kh√°i ni·ªám c∆° b·∫£n

**Self-Attention** = c√°ch AI x√°c ƒë·ªãnh "ƒëi·ªÉm ·∫£nh n√†o quan tr·ªçng v·ªõi nhau"

V√≠ d·ª• trong c√¢u ti·∫øng Anh:
```
"The dog saw the cat and it ran away"
     ‚Üë                        ‚Üë
  T·ª´ "it" n√™n ch√∫ √Ω t·ªõi t·ª´ "dog" (kh√¥ng ph·∫£i "cat")
```

T∆∞∆°ng t·ª±, trong ·∫£nh:
```
Khi v·∫Ω chi ti·∫øt m·∫Øt m√®o ‚Üí ch√∫ √Ω t·ªõi pixel m·∫Øt + v√πng xung quanh
Kh√¥ng c·∫ßn ch√∫ √Ω t·ªõi background (c√¢y, b·∫ßu tr·ªùi)
```

### C∆° ch·∫ø to√°n h·ªçc

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**Ba th√†nh ph·∫ßn:**

1. **Query (Q):** "T√¥i ƒëang x·ª≠ l√Ω pixel n√†o?"
2. **Key (K):** "Pixel kh√°c n√†o li√™n quan?"
3. **Value (V):** "L·∫•y th√¥ng tin g√¨ t·ª´ pixel li√™n quan?"

**V√≠ d·ª• c·ª• th·ªÉ:**

```
Input: ·∫¢nh 64√ó64√ó256 channels (t·ª´ CNN layer)

Step 1: T·∫°o Q, K, V t·ª´ feature map
  Q = Linear_Q(feature_map)    # 4096√ó256 ‚Üí 4096√ó64
  K = Linear_K(feature_map)    # 4096√ó256 ‚Üí 4096√ó64
  V = Linear_V(feature_map)    # 4096√ó256 ‚Üí 4096√ó256

Step 2: T√≠nh s·ª± t∆∞∆°ng ƒë·ªìng
  scores = Q @ K^T             # 4096√ó4096 (m·ªói pixel v·ªõi t·∫•t c·∫£ pixel kh√°c)

Step 3: Chu·∫©n h√≥a
  weights = softmax(scores)    # T·ªïng b·∫±ng 1 m·ªói d√≤ng

Step 4: Tr√≠ch th√¥ng tin
  output = weights @ V         # 4096√ó256 (th√¥ng tin c√≥ tr·ªçng s·ªë)
```

### T·∫°i sao quan tr·ªçng trong b√†i to√°n n√†y?

**Trong FeatGLaC** (Feature-Guided Layer Compositing):

```
Original U-Net Self-Attention:
  Attention(Q, K, V) 
  ‚Üì
  T·∫°o ·∫£nh d·ª±a v√†o noise + diffusion timestep

Modified (Feature Injection):
  Attention(Q, K_guided, V_guided)
  ‚Üì
  √âp bu·ªôc t·∫°o ·∫£nh tu√¢n theo c·∫•u tr√∫c layers ƒë∆∞·ª£c t√°ch
```

**K·∫øt qu·∫£:** 
- Model t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh √°nh s√°ng, b√≥ng ƒë·ªï
- Kh√¥ng c·∫ßn hard merge gi·ªØa ti·ªÅn/h·∫≠u c·∫£nh
- √Ånh s√°ng t·ª± nhi√™n v√¨ diffusion process t·ª± kh√¥i ph·ª•c harmonic

---

## 0.3 CHI TI·∫æT: FEATURE INJECTION (Ti√™m ƒë·∫∑c tr∆∞ng)

### Kh√°i ni·ªám c∆° b·∫£n

**Feature Injection** = can thi·ªáp v√†o qu√° tr√¨nh t√≠nh to√°n c·ªßa m√¥ h√¨nh ƒë·ªÉ √©p n√≥ tu√¢n theo m·ªôt c·∫•u tr√∫c

V√≠ d·ª• trong ƒë·ªùi s·ªëng:
```
B√¨nh th∆∞·ªùng: M√°y t·ª± v·∫Ω ·∫£nh t·ª± do
V·ªõi injection: "H√£y v·∫Ω nh∆∞ng b·∫Øt bu·ªôc ph·∫£i c√≥ c√°i c√¢y ·ªü g√≥c tr√°i"
```

### C√°ch ho·∫°t ƒë·ªông trong FeatGLaC

**B∆∞·ªõc 1: Chu·∫©n b·ªã Guidance Features**

```
Ti·ªÅn c·∫£nh Layer + H·∫≠u c·∫£nh Layer
    ‚Üì
VAE Encoder ‚Üí Latent representations
    ‚Üì
Guidance U-Net (m√¥ h√¨nh ph·ª•)
    ‚Üì
Tr√≠ch features t·ª´ m·ªói attention layer
    K_fg, V_fg = from foreground
    K_bg, V_bg = from background
```

**B∆∞·ªõc 2: Injection v√†o Generation U-Net**

```
Diffusion loop (t = T ƒë·∫øn 0):
    ‚Üì
  noise_t = model(z_t, t, text_prompt)  ‚Üê b√¨nh th∆∞·ªùng
  ‚Üì
  T·∫°i m·ªói Self-Attention layer:
    
    Q_gen = from generation U-Net (t√≠nh b√¨nh th∆∞·ªùng)
    K_gen, V_gen = b√¨nh th∆∞·ªùng
    
    Thay th·∫ø:
    K_gen ‚Üê blend(K_fg, K_bg)  ‚Üê d√πng guidance features!
    V_gen ‚Üê blend(V_fg, V_bg)
    
    output = Attention(Q_gen, K_gen_injected, V_gen_injected)
```

**B∆∞·ªõc 3: K·∫øt qu·∫£**

```
output = DecoderUNet(...)
    ‚Üì
VAE Decoder ‚Üí ·∫£nh cu·ªëi c√πng
    ‚Üì
·∫¢nh v·ªõi:
  ‚úÖ C·∫•u tr√∫c gi·ªëng ti·ªÅn/h·∫≠u c·∫£nh
  ‚úÖ √Ånh s√°ng t·ª± ƒë·ªông c√¢n ch·ªânh
  ‚úÖ Chi ti·∫øt m∆∞·ª£t m√† (kh√¥ng c·∫°nh c·ª©ng)
```

### T·∫°i sao ho·∫°t ƒë·ªông t·ªët?

1. **Self-Attention** h·ªçc "m·ªëi quan h·ªá to√†n c·∫£nh"
   - Bi·∫øt √°nh s√°ng n√™n t·ª´ ƒë√¢u
   - Bi·∫øt b√≥ng ƒë·ªï n√™n ·ªü ch·ªó n√†o
   
2. **Diffusion Process** = l·∫∑p 50 b∆∞·ªõc kh·ª≠ nhi·ªÖu
   - Kh√¥ng l√†m m·ªôt l·∫ßn (ki·ªÉu hard merge)
   - T·ª´ t·ª´ ƒëi·ªÅu ch·ªânh, c√°c chi ti·∫øt kh√¥ng nh·∫•t qu√°n t·ª± kh√¥i ph·ª•c

3. **Feature Injection** = soft constraint
   - Kh√¥ng b·∫Øt bu·ªôc tu√¢n theo 100% (s·∫Ω c·ª©ng, gi·∫£ t·∫°o)
   - √âp bu·ªôc nh∆∞ng ƒë·ªÉ m√¥ h√¨nh linh ho·∫°t ƒëi·ªÅu ch·ªânh

### V√≠ d·ª• th·ª±c t·∫ø

**Ch√®n gh·∫ø v√†o ·∫£nh ph√≤ng:**

```
Ti·ªÅn c·∫£nh: C√°i b√†n
H·∫≠u c·∫£nh: T∆∞·ªùng, c·ª≠a, s√†n

B√¨nh th∆∞·ªùng (hard merge):
  Gh·∫ø d√°n l√™n b√†n ‚Üí n√≥ tr√¥ng s√°ng b·∫•t th∆∞·ªùng (kh√¥ng b√≥ng)

V·ªõi Feature Injection:
  1. Guidance features n√≥i: "ƒê√¢y l√† v√πng b√†n, v√πng t∆∞·ªùng"
  2. Generation U-Net sinh gh·∫ø nh∆∞ng inject guidance
  3. Self-Attention layers t·ª± ƒë·ªông:
     - Gh·∫ø ·ªü ph√≠a sau b√†n (occlusion)
     - √Ånh s√°ng t·ª´ c·ª≠a ‚Üí gh·∫ø t·ªëi ph√≠a m·ªôt
     - B√≥ng ƒë·ªï h·ª£p l√Ω tr√™n s√†n
  4. K·∫øt qu·∫£: t·ª± nhi√™n, kh√¥ng c·∫ßn post-processing
```

---

## 1. PH√ÇN T√çCH V·∫§N ƒê·ªÄ (PROBLEM STATEMENT)

### 1.1 V·∫•n ƒë·ªÅ hi·ªán t·∫°i

C√°c c√¥ng c·ª• ch·ªânh s·ª≠a ·∫£nh AI (Photoshop Generative Fill, inpainting) **ho·∫°t ƒë·ªông tr√™n m·∫∑t ph·∫≥ng 2D** - kh√¥ng hi·ªÉu quan h·ªá kh√¥ng gian 3D.

**V√≠ d·ª• c·ª• th·ªÉ:**
- Khi ch√®n c√°i gh·∫ø v√†o ·∫£nh: d√°n gh·∫ø ƒë√® l√™n m·ªçi th·ª© (kh√¥ng bi·∫øt gh·∫ø n·∫±m sau b√†n)
- √Ånh s√°ng kh√¥ng t·ª± nhi√™n (gh·∫ø s√°ng trong khi b√†n t·ªëi)
- Kh√¥ng t·∫°o b√≥ng ƒë·ªï ph√π h·ª£p

**H·∫≠u qu·∫£:** ·∫¢nh nh√¨n gi·∫£ t·∫°o, sai ph·ªëi c·∫£nh, m·∫•t t√≠nh logic v·∫≠t l√Ω.

### 1.2 Gi·∫£i ph√°p

B√†i b√°o ICCV 2025 ƒë·ªÅ xu·∫•t quy tr√¨nh **DeGLaD + FeatGLaC**:
1. **DeGLaD:** T√°ch ·∫£nh th√†nh c√°c l·ªõp (ti·ªÅn c·∫£nh/h·∫≠u c·∫£nh) d·ª±a tr√™n ƒë·ªô s√¢u
2. **FeatGLaC:** Gh√©p l·∫°i v·ªõi √°nh s√°ng t·ª± nhi√™n b·∫±ng Feature Injection

**T·∫•t c·∫£ ƒë·ªÅu Zero-Shot:** Kh√¥ng c·∫ßn hu·∫•n luy·ªán m√¥ h√¨nh m·ªõi.

### 1.3 K·∫øt qu·∫£ mong ƒë·ª£i

‚úÖ ·∫¢nh gh√©p tu√¢n th·ªß quy lu·∫≠t 3D  
‚úÖ √Ånh s√°ng v√† b√≥ng ƒë·ªï t·ª± nhi√™n, h√†i h√≤a  
‚úÖ Kh√¥ng c·∫ßn qu√° tr√¨nh h·∫≠u x·ª≠ l√Ω ph·ª©c t·∫°p

---

## 2. C√ÅC TH√ÄNH PH·∫¶N C·ªêT L√ïI (CORE COMPONENTS)

H·ªá th·ªëng l√† **pipeline k·∫øt h·ª£p** nhi·ªÅu m√¥ h√¨nh pre-trained, kh√¥ng ph·∫£i m√¥ h√¨nh ƒë∆°n l·∫ª.

### 2.1 Latent Diffusion Model (LDM) - X∆∞∆°ng s·ªëng

**B·∫£n ch·∫•t:** M√¥ h√¨nh sinh (Generative Model) x√°c su·∫•t

**M√¥ h√¨nh c·ª• th·ªÉ:** Stable Diffusion ho·∫∑c bi·∫øn th·ªÉ

**C∆° ch·∫ø:**
- **Qu√° tr√¨nh Khu·∫øch t√°n:** Th√™m nhi·ªÖu v√†o ·∫£nh g·ªëc t·ª´ t·ª´
- **Qu√° tr√¨nh Kh·ª≠ Nhi·ªÖu:** H·ªçc c√°ch lo·∫°i b·ªè nhi·ªÖu ƒë·ªÉ kh√¥i ph·ª•c ·∫£nh r√µ n√©t
- **Latent Space:** Ho·∫°t ƒë·ªông tr√™n kh√¥ng gian ƒë·∫∑c tr∆∞ng (4-8 l·∫ßn nh·ªè h∆°n pixel)

**Tham s·ªë:** ~1.5 t·ª∑

**L·ª£i √≠ch:**
- Kh·∫£ nƒÉng sinh ·∫£nh ƒëa d·∫°ng
- Ch·∫•t l∆∞·ª£ng cao, chi ti·∫øt ƒë·∫πp
- C√≥ th·ªÉ h∆∞·ªõng d·∫´n b·∫±ng text ho·∫∑c image guidance

### 2.2 Depth Estimator - C·∫£m bi·∫øn 3D

**B·∫£n ch·∫•t:** M·∫°ng CNN/Transformer chuy√™n bi·ªát d·ª± ƒëo√°n ƒë·ªô s√¢u

**M√¥ h√¨nh:** MiDaS ho·∫∑c ZoeDepth

**Ch·ª©c nƒÉng:**
- Input: ·∫¢nh 2D (RGB)
- Output: Depth Map (gi√° tr·ªã 0-255 ho·∫∑c 0-1)
  - Gi√° tr·ªã cao = xa (n·ªÅn)
  - Gi√° tr·ªã th·∫•p = g·∫ßn (ti·ªÅn c·∫£nh)

**ƒê·ªô ch√≠nh x√°c:** T∆∞∆°ng ƒë·ªëi t·ªët tr√™n ·∫£nh th·ª±c t·∫ø

**Gi·ªõi h·∫°n:**
- L√† relative depth, kh√¥ng metric depth
- C√≥ th·ªÉ sai tr√™n v·∫≠t th·ªÉ trong su·ªët, b√≥ng ƒë·ªï

### 2.3 Identity Preserver (AnyDoor) - Gi·ªØ nh√¢n d·∫°ng

**B·∫£n ch·∫•t:** M√¥ h√¨nh Reference-Guided Generation

**Ch·ª©c nƒÉng:** ƒê·∫£m b·∫£o v·∫≠t th·ªÉ kh√¥ng b·ªã bi·∫øn d·∫°ng, gi·ªØ nguy√™n nh√¢n d·∫°ng g·ªëc

**C∆° ch·∫ø:**
1. **Encoding:** M√£ h√≥a ·∫£nh tham chi·∫øu th√†nh feature vector $F_{ref}$
2. **Injection:** Ti√™m $F_{ref}$ v√†o qu√° tr√¨nh sinh ·∫£nh c·ªßa U-Net
3. **√âp bu·ªôc:** U-Net ph·∫£i s·ª≠ d·ª•ng $F_{ref}$, n√™n k·∫øt qu·∫£ gi·ªØ nguy√™n nh√¢n d·∫°ng

**L·ª£i √≠ch:**
- V·∫≠t th·ªÉ kh√¥ng m√©o m√≥, bi·∫øn h√¨nh
- Gi·ªØ l·∫°i chi ti·∫øt g·ªëc
- H·ªó tr·ª£ v·∫≠t ph·ª©c t·∫°p

### 2.4 VAE Encoder/Decoder - C·∫ßu n·ªëi

**B·∫£n ch·∫•t:** Variational Autoencoder

**Ch·ª©c nƒÉng:**
- **Encoder:** ·∫¢nh (h√†ng tri·ªáu gi√° tr·ªã) ‚Üí latent space (h√†ng trƒÉm gi√° tr·ªã)
- **Decoder:** Latent space ‚Üí ·∫£nh ƒë·ªÉ hi·ªÉn th·ªã

**T·∫°i sao c·∫ßn:**
- Gi·∫£m chi ph√≠ t√≠nh to√°n 4-8 l·∫ßn
- T·∫≠p trung v√†o ƒë·∫∑c tr∆∞ng quan tr·ªçng
- TƒÉng t·ªëc ƒë·ªô sinh ·∫£nh

---

## 3. KI·∫æN TR√öC H·ªÜ TH·ªêNG (SYSTEM ARCHITECTURE)

H·ªá th·ªëng ho·∫°t ƒë·ªông theo quy tr√¨nh **tu·∫ßn t·ª± 2 b∆∞·ªõc ch√≠nh.**

### 3.1 B∆∞·ªõc 1: DeGLaD (Depth-Guided Layer Decomposition)

**Ch·ª©c nƒÉng:** T√°ch ·∫£nh th√†nh c√°c l·ªõp kh√¥ng gian d·ª±a tr√™n ƒë·ªô s√¢u

#### Quy tr√¨nh chi ti·∫øt

**Input:**
- ·∫¢nh g·ªëc (RGB)
- B·∫£n ƒë·ªì ƒë·ªô s√¢u (t·ª´ MiDaS)
- Ng∆∞·ª°ng ƒë·ªô s√¢u $d$ (ch·ªçn b·ªüi ng∆∞·ªùi d√πng)

**B∆∞·ªõc 1: ∆Ø·ªõc l∆∞·ª£ng Depth Map**
```
depth_map = MiDaS(rgb_image)  # K·∫øt qu·∫£ 0-1 ho·∫∑c 0-255
```

**B∆∞·ªõc 2: T·∫°o m·∫∑t n·∫°**
```
mask_foreground = depth_map < d       # Pixels g·∫ßn h∆°n ng∆∞·ª°ng
mask_background = depth_map >= d      # Pixels xa h∆°n ng∆∞·ª°ng
```

**B∆∞·ªõc 3: T√°ch l·ªõp**
```
layer_fg = rgb_image * mask_foreground
layer_bg = rgb_image * mask_background
```

**B∆∞·ªõc 4: Inpainting l·ªó th·ªßng**
- L·ªõp H·∫≠u c·∫£nh c√≥ "l·ªó ƒëen" ·ªü n∆°i Ti·ªÅn c·∫£nh che khu·∫•t
- K√≠ch ho·∫°t m√¥ h√¨nh Inpainting ƒë·ªÉ v·∫Ω b√π
- S·ª≠ d·ª•ng Diffusion inpainting ho·∫∑c CNN inpainting

**Output:**
- Layer Ti·ªÅn c·∫£nh (s·∫°ch, s·∫µn s√†ng ch·ªânh s·ª≠a)
- Layer H·∫≠u c·∫£nh (s·∫°ch, kh√¥ng l·ªó)

#### ∆Øu ƒëi·ªÉm

‚úÖ **ƒê∆°n gi·∫£n:** Ch·ªâ c·∫ßn thresholds  
‚úÖ **Nhanh:** To√†n x·ª≠ l√Ω nguy√™n l√Ω h·ªçc  
‚úÖ **ƒêi·ªÅu ch·ªânh d·ªÖ:** Ng∆∞·ªùi d√πng ch·ªçn ng∆∞·ª°ng

#### H·∫°n ch·∫ø

‚ùå **Ph·ª• thu·ªôc Depth Map:** N·∫øu depth sai ‚Üí k·∫øt qu·∫£ sai  
‚ùå **C·∫°nh c·ª©ng:** Ranh gi·ªõi ti·ªÅn/h·∫≠u c·∫£nh b·ªã s·∫Øc  
‚ùå **Occlusion edge:** Kh√≥ x·ª≠ l√Ω c·∫°nh m·ªèng, t√≥c

### 3.2 B∆∞·ªõc 2: FeatGLaC (Feature-Guided Layer Compositing)

**Ch·ª©c nƒÉng:** Gh√©p c√°c l·ªõp l·∫°i th√†nh ·∫£nh ho√†n ch·ªânh v·ªõi √°nh s√°ng t·ª± nhi√™n

#### V·∫•n ƒë·ªÅ v·ªõi Alpha Blending

Gh√©p ch·ªìng pixel ƒë∆°n gi·∫£n:
```
output = fg * alpha + bg * (1 - alpha)
```

**K·∫øt qu·∫£ t·ªìi:**
- Vi·ªÅn s·∫Øc ngo·∫∑c
- √Ånh s√°ng kh√¥ng ƒÉn nh·∫≠p
- B√≥ng ƒë·ªï kh√¥ng t·ª± nhi√™n

#### Gi·∫£i ph√°p: Feature Injection

**Thay v√¨ gh√©p pixel, can thi·ªáp v√†o b·ªô n√£o (U-Net)** c·ªßa m√¥ h√¨nh Diffusion

**Ki·∫øn tr√∫c hai nh√°nh:**

**Nh√°nh 1 - Guidance Branch:**
```
input_layers = [layer_fg, layer_bg]
‚Üì
VAE Encoder ‚Üí Latent vectors
‚Üì
Guidance U-Net ‚Üí x·ª≠ l√Ω
‚Üì
Internal Features: K (Key), V (Value)
  K = c·∫•u tr√∫c h√¨nh h·ªçc
  V = th√¥ng tin m√†u s·∫Øc
```

**Nh√°nh 2 - Generation Branch:**
```
noise_z ~ N(0, 1)
‚Üì
Generation U-Net (T b∆∞·ªõc kh·ª≠ nhi·ªÖu)
  At each step t:
    Inject K, V v√†o Self-Attention layers
    ‚Üí √âp bu·ªôc sinh ·∫£nh tu√¢n theo c·∫•u tr√∫c guidance
‚Üì
VAE Decoder ‚Üí ·∫¢nh cu·ªëi c√πng
```

#### T·∫°i sao ho·∫°t ƒë·ªông t·ªët?

1. **K (Geometric Structure):** C·∫•u tr√∫c ·∫£nh sinh gi·ªëng h·ªát layers ƒë√£ t√°ch
2. **V (Appearance):** Gi·ªØ l·∫°i m√†u s·∫Øc v√† chi ti·∫øt g·ªëc
3. **Self-Attention Injection:** √Ånh s√°ng AI t·ª± ƒë·ªông c√¢n ch·ªânh (kh√¥ng pixel c·ª©ng nh·∫Øc)
4. **Diffusion Process:** Qua nhi·ªÅu b∆∞·ªõc, c√°c kh√¥ng nh·∫•t qu√°n ƒë∆∞·ª£c gi·∫£i quy·∫øt t·ª± nhi√™n

#### C√¥ng th·ª©c to√°n h·ªçc

$$\text{Attention}(Q, K_{\text{guided}}, V_{\text{guided}}) = \text{softmax}\left(\frac{QK_{\text{guided}}^T}{\sqrt{d}}\right)V_{\text{guided}}$$

Thay th·∫ø $K$ v√† $V$ g·ªëc b·∫±ng c√°c t·ª´ nh√°nh h∆∞·ªõng d·∫´n.

---

## 4. TRI·ªÇN KHAI H·ªÜ TH·ªêNG BIG DATA (IMPLEMENTATION)

C√°ch √°p d·ª•ng thu·∫≠t to√°n ƒë·ªÉ x·ª≠ l√Ω **10GB d·ªØ li·ªáu** (~25,000 ·∫£nh) tr√™n Kaggle.

### 4.1 Th√°ch th·ª©c & Kh·∫Øc ph·ª•c

| Th√°ch th·ª©c | Nguy√™n nh√¢n | Gi·∫£i ph√°p |
|-----------|----------|---------|
| 10GB kh√¥ng v√†o RAM | RAM 32GB, CPU c≈©ng c·∫ßn kh√¥ng gian | Streaming DataLoader - batch 32 ·∫£nh |
| X·ª≠ l√Ω tu·∫ßn t·ª± qu√° l√¢u | 1 GPU √ó 2.5s √ó 25K = 70+ gi·ªù | Data Parallelism - 2 GPU T4 |
| M√¥ h√¨nh 1.5B params | GPU 16GB VRAM, m√¥ h√¨nh c·∫ßn ~6GB | Batch nh·ªè (32), Mixed Precision FP16 |
| L∆∞u 25K ·∫£nh output | Dung l∆∞·ª£ng ~20-30GB | Write li√™n t·ª•c, kh√¥ng gi·ªØ RAM |

### 4.2 Ki·∫øn tr√∫c Batch Processing Pipeline

#### L·ªõp 1: Data Storage

**Input g·ªëc:** Video 4K t·ª± quay

**Ti·ªÅn x·ª≠ l√Ω:**
1. C·∫Øt video th√†nh frame (1 frame/0.1s)
2. K·∫øt qu·∫£: ~25,000 ·∫£nh PNG/JPEG
3. L·ª£i √≠ch: D·ªØ li·ªáu l·ªõn, ch·∫•t l∆∞·ª£ng cao, ƒë·ªìng nh·∫•t

#### L·ªõp 2: Data Controller

**Ch·ª©c nƒÉng:** Qu·∫£n l√Ω lu·ªìng d·ªØ li·ªáu disk ‚Üí RAM ‚Üí GPU

**Th√†nh ph·∫ßn:**

1. **File Manager:** Qu√©t 25,000 ·∫£nh, t·∫°o danh s√°ch
2. **DataLoader:** N·∫°p cu·ªën chi·∫øu
   - 1 batch = 32 ·∫£nh v√†o RAM
   - Sau x·ª≠ l√Ω, x√≥a kh·ªèi RAM ‚Üí n·∫°p batch ti·∫øp
   - Memory: ~500MB-1GB/l√∫c
3. **Preprocessing:**
   - Resize v·ªÅ 512√ó512 ho·∫∑c 768√ó768
   - Normalize (0-1 ho·∫∑c -1 to 1)
   - Chu·∫©n b·ªã tensor

**Code logic:**
```
for batch in DataLoader(images, batch_size=32):
    process(batch)
    save_results(batch)
    del batch  # Gi·∫£i ph√≥ng RAM
```

#### L·ªõp 3: Compute Cluster

**C·∫•u h√¨nh:**
- 2 √ó NVIDIA T4 (16GB VRAM m·ªói)
- 8-core CPU
- 32GB system RAM

**Chia vi·ªác:**
```
batch = [img1, img2, ..., img32]
  ‚Üì
Split:
  Part 1 (16 ·∫£nh) ‚Üí GPU 0
  Part 2 (16 ·∫£nh) ‚Üí GPU 1
  ‚Üì
GPU 0 & GPU 1: DeGLaD + FeatGLaC (song song)
  ‚Üì
Synchronize ‚Üí k·∫øt qu·∫£ gom
```

**T·ªëc ƒë·ªô:**
- 1 ·∫£nh = ~2.5 gi√¢y
- 2 GPU song song = ~1.25 gi√¢y/·∫£nh (l√Ω t∆∞·ªüng)
- Th·ª±c t·∫ø: ~1.8 gi√¢y/·∫£nh
- **T·ªïng:** 25,000 √ó 1.8s √∑ 3600 ‚âà **12-14 gi·ªù**

**Workflow tr√™n m·ªói GPU:**
```
for each image in batch:
    1. Load image ‚Üí tensor
    
    2. DeGLaD:
       - Depth = MiDaS(image)
       - Mask_fg = depth < threshold
       - Layer_fg, Layer_bg = separate(image, mask)
       - Layer_bg = Inpaint(layer_bg)
    
    3. FeatGLaC:
       - Guidance_feats = Guidance_UNet(layer_fg, layer_bg)
       - Output = Generation_UNet_with_Injection(guidance_feats)
    
    4. Save output
```

#### L·ªõp 4: Aggregation & Evaluation

**ƒê·∫ßu v√†o:** 25,000 ·∫£nh output

**B∆∞·ªõc 1: Validation**
```
Check file size, format, dimension
Skip corrupted files
```

**B∆∞·ªõc 2: Metric Calculation (RMSE)**
```
For each image:
    predicted_depth = MiDaS(output_image)
    true_depth = ground_truth[image_id]
    rmse = sqrt(mean((predicted - true)^2))
    
Final_RMSE = mean(all_rmse)
```

**B∆∞·ªõc 3: Visualization**
- Bi·ªÉu ƒë·ªì RMSE qua image
- Histogram RMSE
- Top-K worst/best cases

### 4.3 C·∫•u h√¨nh Ph·∫ßn c·ª©ng & T·ªëi ∆∞u

| Th√†nh ph·∫ßn | Chi ti·∫øt | T√°c d·ª•ng |
|-----------|---------|---------|
| **GPU** | 2√ó T4 (16GB VRAM) | X·ª≠ l√Ω DNN |
| **CPU** | 8-core Xeon | Data loading |
| **RAM** | 32GB DDR4 | Buffer |
| **Storage** | 100GB NVMe SSD | Input/Output |
| **Batch Size** | 32 | C√¢n b·∫±ng memory/throughput |
| **DataLoader Workers** | 8 | Parallel loading |
| **Mixed Precision** | FP16 | Gi·∫£m memory 2√ó, tƒÉng t·ªëc 1.5-2√ó |
| **Processing Time** | ~12-14 gi·ªù | Th·ªùi gian th·ª±c t·∫ø |

### 4.4 T√≥m t·∫Øt lu·ªìng d·ªØ li·ªáu

```
1. Video 4K
   ‚Üì
2. Frame Extraction (25,000 ·∫£nh)
   ‚Üì
3. DataLoader (batch=32)
   ‚Üì
4. GPU-0: DeGLaD+FeatGLaC (16) | GPU-1: DeGLaD+FeatGLaC (16)
   ‚Üì                             ‚Üì
5. Merge results
   ‚Üì
6. Save to SSD (~20-30GB)
   ‚Üì
7. Metric Calculation (RMSE)
   ‚Üì
8. Report & Visualization
```

---

## 5. H√ÄM M·∫§T M√ÅT & H·ªåC (LOSS FUNCTIONS)

> **L∆∞u √Ω:** Zero-Shot (Inference-Only) ‚Üí kh√¥ng training tr·ª±c ti·∫øp. Tuy nhi√™n m√¥ h√¨nh pre-train v·ªõi:

### 5.1 Noise Prediction Loss (MSE)

$$\mathcal{L}_{denoise} = \mathbb{E}_{x_0, t, \epsilon} \left[\|\epsilon - \epsilon_\theta(x_t, t)\|_2^2\right]$$

**√ù nghƒ©a:** M√¥ h√¨nh h·ªçc d·ª± ƒëo√°n l·ªõp nhi·ªÖu ƒë∆∞·ª£c th√™m v√†o ·∫£nh t·∫°i th·ªùi ƒëi·ªÉm $t$.

### 5.2 Perceptual Loss (LPIPS)

$$\mathcal{L}_{perceptual} = \sum_l \frac{1}{N_l} \sum_{h,w} \|F_l(x) - F_l(y)\|_2^2$$

**√ù nghƒ©a:** So s√°nh ·ªü m·ª©c ƒë·∫∑c tr∆∞ng (kh√¥ng ph·∫£i pixel). ƒê·∫£m b·∫£o ·∫£nh sinh "th·∫≠t" theo tri gi√°c con ng∆∞·ªùi.

### 5.3 Feature Matching Loss

$$\mathcal{L}_{feat-match} = \|F_{ref} - F_{generated}\|_2$$

**√ù nghƒ©a:** √âp bu·ªôc ƒë·∫∑c tr∆∞ng v·∫≠t th·ªÉ gi·ªØ nguy√™n, kh√¥ng m√©o m√≥.

### 5.4 Reconstruction Loss (L1/L2)

$$\mathcal{L}_{recon} = \|x_{0} - \hat{x}_{0}\|_1$$

**√ù nghƒ©a:** ·∫¢nh kh√¥i ph·ª•c g·∫ßn v·ªõi ·∫£nh g·ªëc.

---

## 6. T√ìM T·∫ÆT & K·ª≤ V·ªåNG

### ∆Øu ƒëi·ªÉm

‚úÖ **Zero-Shot:** Kh√¥ng c·∫ßn hu·∫•n luy·ªán m√¥ h√¨nh m·ªõi  
‚úÖ **3D-Aware:** X·ª≠ l√Ω occlusion, ph·ªëi c·∫£nh ƒë√∫ng  
‚úÖ **√Ånh s√°ng t·ª± nhi√™n:** Feature Injection t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh  
‚úÖ **M·ªü r·ªông:** √Åp d·ª•ng tr√™n c√°c t√°c v·ª• kh√°c

### H·∫°n ch·∫ø

‚ùå **Ph·ª• thu·ªôc depth map:** Depth sai ‚Üí k·∫øt qu·∫£ sai  
‚ùå **T·ªëc ƒë·ªô:** 2.5s/·∫£nh  
‚ùå **T√†i nguy√™n:** GPU ‚â•16GB VRAM  
‚ùå **Chi ti·∫øt nh·ªè:** C√≥ th·ªÉ lose t√≥c, c·∫°nh m·ªèng

### H∆∞·ªõng ph√°t tri·ªÉn

üîÆ **Depth Improvement:** ZoeDepth, Metric Depth  
üîÆ **Speed Optimization:** Quantization, distillation  
üîÆ **Generalization:** Video editing (frame-consistent)  
üîÆ **User Control:** Interactive UI ƒëi·ªÅu ch·ªânh threshold

---

## T√†i li·ªáu tham kh·∫£o

- Parihar et al., ICCV 2025
- Rombach et al., CVPR 2022  
- Xia et al., arXiv 2023 (AnyDoor)
- MiDaS, ZoeDepth
