{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2002504,"sourceType":"datasetVersion","datasetId":1198025}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hongtptrai/big-data?scriptVersionId=288926482\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"**Library and Initial Enviroment**","metadata":{}},{"cell_type":"code","source":"import shutil\nimport subprocess\nimport os\nimport sys\nimport io\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport cv2\nimport time\nfrom PIL import Image\nfrom torchvision.transforms import Compose\nfrom pyspark.sql.functions import col \nfrom pyspark.sql import SparkSession\nfrom pyspark import TaskContext\n\nprint(\"=== BẮT ĐẦU SETUP ===\")\n\n# --- 1. CLONE REPO ---\nrepo_dir = \"/kaggle/working/Depth-Aware-Editing\"\n\n# Kiểm tra folder chưa có hoặc rỗng thì mới clone\nshould_clone = False\nif not os.path.exists(repo_dir):\n    should_clone = True\nelif not os.listdir(repo_dir):\n    should_clone = True\n\nif should_clone:\n    print(\"1. Đang Clone Repo 'Depth-Aware-Editing'...\")\n    subprocess.run([\"git\", \"clone\", \"https://github.com/rishubhpar/Depth-Aware-Editing\", repo_dir])\nelse:\n    print(\"1. Repo chính đã có và không rỗng.\")\n\n# --- 2. SETUP DINOv2 ---\nhub_dir = \"/kaggle/working/torchhub\"\ndinov2_path = os.path.join(hub_dir, \"facebookresearch_dinov2_main\")\n\nif not os.path.exists(dinov2_path):\n    print(\"2. Đang cài đặt DINOv2...\")\n    os.makedirs(hub_dir, exist_ok=True)\n    subprocess.run([\"wget\", \"-q\", \"-O\", \"dinov2.zip\", \"https://github.com/facebookresearch/dinov2/archive/refs/heads/main.zip\"])\n    subprocess.run([\"unzip\", \"-q\", \"-o\", \"dinov2.zip\", \"-d\", hub_dir])\n    \n    extracted_path = os.path.join(hub_dir, \"dinov2-main\")\n    if os.path.exists(extracted_path):\n        if os.path.exists(dinov2_path):\n            shutil.rmtree(dinov2_path)\n        os.rename(extracted_path, dinov2_path)\n    \n    if os.path.exists(\"dinov2.zip\"):\n        os.remove(\"dinov2.zip\")\n    print(\"   -> DINOv2 Ready.\")\nelse:\n    print(\"2. DINOv2 đã có sẵn.\")\n\n# --- 3. TẢI WEIGHTS ---\nweights_dir = os.path.join(repo_dir, \"weights\")\nos.makedirs(weights_dir, exist_ok=True)\nweights_path = os.path.join(weights_dir, \"depth_anything_vits14.pth\")\nurl = \"https://huggingface.co/spaces/LiheYoung/Depth-Anything/resolve/main/checkpoints/depth_anything_vits14.pth\"\n\n# Xóa file lỗi (<1MB)\nif os.path.exists(weights_path):\n    if os.path.getsize(weights_path) < 1_000_000:\n        os.remove(weights_path)\n\nif not os.path.exists(weights_path):\n    print(\"3. Đang tải Weights (VITS)...\")\n    subprocess.run([\"curl\", \"-L\", \"-o\", weights_path, url])\n\n# Kiểm tra kết quả\nif os.path.exists(weights_path) and os.path.getsize(weights_path) > 1_000_000:\n    sz = os.path.getsize(weights_path) / (1024 * 1024)\n    print(f\"✅ SETUP HOÀN TẤT! Weights: {sz:.2f} MB\")\nelse:\n    print(\"❌ Lỗi tải Weights. Hãy kiểm tra Internet!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T17:10:24.518407Z","iopub.execute_input":"2025-12-28T17:10:24.519059Z","iopub.status.idle":"2025-12-28T17:10:24.530124Z","shell.execute_reply.started":"2025-12-28T17:10:24.519028Z","shell.execute_reply":"2025-12-28T17:10:24.529403Z"}},"outputs":[{"name":"stdout","text":"=== BẮT ĐẦU SETUP ===\n1. Repo chính đã có và không rỗng.\n2. DINOv2 đã có sẵn.\n✅ SETUP HOÀN TẤT! Weights: 94.62 MB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"CONFIG = {\n    \"REPO_PATH\": \"/kaggle/working/Depth-Aware-Editing/src/Depth-Anything\",\n    \n    \"INPUT_DATA_PATH\": \"/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train/*\", \n    \n    \"OUTPUT_DATA_PATH\": \"/kaggle/working/depth_output\",\n}\n\nif not os.path.exists(CONFIG[\"OUTPUT_DATA_PATH\"]):\n    os.makedirs(CONFIG[\"OUTPUT_DATA_PATH\"])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T17:10:24.531825Z","iopub.execute_input":"2025-12-28T17:10:24.532127Z","iopub.status.idle":"2025-12-28T17:10:24.542901Z","shell.execute_reply.started":"2025-12-28T17:10:24.532108Z","shell.execute_reply":"2025-12-28T17:10:24.5423Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**Core Logic**","metadata":{"execution":{"iopub.status.busy":"2025-12-28T17:07:52.481504Z","iopub.execute_input":"2025-12-28T17:07:52.481673Z","iopub.status.idle":"2025-12-28T17:07:52.493345Z","shell.execute_reply.started":"2025-12-28T17:07:52.481659Z","shell.execute_reply":"2025-12-28T17:07:52.492286Z"}}},{"cell_type":"code","source":"def process_partition(iterator):\n    import shutil\n    import subprocess\n    import os\n    import sys\n    import io\n    import torch\n    import torch.nn.functional as F\n    import numpy as np\n    import cv2\n    import time\n    from PIL import Image\n    from torchvision.transforms import Compose\n    from pyspark.sql.functions import col \n    from pyspark.sql import SparkSession\n    from pyspark import TaskContext\n    import random\n\n    \n    # --- 1. DESYNC: Ngủ ngẫu nhiên để tránh khởi động cùng lúc ---\n    time.sleep(random.uniform(0.5, 3.0))\n    \n    # Setup Path\n    repo_path = \"/kaggle/working/Depth-Aware-Editing/src/Depth-Anything\"\n    if repo_path not in sys.path: sys.path.append(repo_path)\n    \n    try:\n        from depth_anything.dpt import DepthAnything\n        from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n    except:\n        sys.path.append(\"/kaggle/working/Depth-Aware-Editing/Depth-Anything\")\n        from depth_anything.dpt import DepthAnything\n        from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n\n    # Chọn GPU\n    ctx = TaskContext.get()\n    part_id = ctx.partitionId()\n    \n    if torch.cuda.is_available():\n        gpu_id = part_id % torch.cuda.device_count()\n        device = torch.device(f\"cuda:{gpu_id}\")\n    else:\n        device = torch.device(\"cpu\")\n\n    # --- 2. RETRY LOGIC: Thử khởi tạo model tối đa 3 lần ---\n    depth_model = None\n    config = { \"encoder\": \"vits\", \"features\": 64, \"out_channels\": [48, 96, 192, 384], \"use_bn\": False, \"use_clstoken\": False }\n    \n    for attempt in range(3):\n        try:\n            # Xóa cache cũ trước khi thử\n            torch.cuda.empty_cache()\n            \n            # Khởi tạo model (Cách chuẩn)\n            model = DepthAnything(config)\n            \n            # Đẩy vào GPU (Đây là bước hay lỗi nhất)\n            depth_model = model.to(device).eval()\n            \n            # Nếu thành công thì thoát vòng lặp\n            break \n        except RuntimeError as e:\n            if \"busy\" in str(e) or \"unavailable\" in str(e):\n                # Nếu GPU bận, chờ 3-5s rồi thử lại\n                time.sleep(3 + random.random())\n                continue\n            else:\n                # Nếu lỗi khác thì báo luôn\n                return iter([(f\"Init_Error_Part_{part_id}\", str(e))])\n        except Exception as e:\n             return iter([(f\"Init_Error_Part_{part_id}\", str(e))])\n\n    if depth_model is None:\n        return iter([(f\"GPU_Busy_Part_{part_id}\", \"Failed to load model after 3 attempts\")])\n\n    # Load Weights\n    ckpt = \"/kaggle/working/Depth-Aware-Editing/weights/depth_anything_vits14.pth\"\n    if os.path.exists(ckpt):\n        depth_model.load_state_dict(torch.load(ckpt, map_location=device), strict=False)\n\n    # Transform\n    transform = Compose([\n        Resize(width=518, height=518, resize_target=False, keep_aspect_ratio=True, ensure_multiple_of=14, resize_method='lower_bound', image_interpolation_method=cv2.INTER_CUBIC),\n        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        PrepareForNet(),\n    ])\n    \n    out_dir = \"/kaggle/working/depth_output\"\n    os.makedirs(out_dir, exist_ok=True)\n    results = []\n    \n    # Batch Processing\n    BATCH_SIZE = 32\n    batch_data, batch_paths, batch_shapes = [], [], []\n\n    def flush_batch():\n        if not batch_data: return\n        try:\n            tensor = torch.from_numpy(np.concatenate(batch_data, axis=0)).to(device)\n            with torch.no_grad(): preds = depth_model(tensor)\n            \n            for i, depth in enumerate(preds):\n                h, w = batch_shapes[i]\n                d_res = F.interpolate(depth[None, None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n                d_np = d_res.cpu().numpy()\n                d_norm = (d_np - d_np.min()) / (d_np.max() - d_np.min()) * 255.0\n                \n                fname = os.path.basename(batch_paths[i]).replace(\".jpg\", \".png\").replace(\".png\", \".png\")\n                Image.fromarray(d_norm.astype(np.uint8)).save(os.path.join(out_dir, fname))\n                results.append((batch_paths[i], \"Success\"))\n        except Exception as e:\n            for p in batch_paths: results.append((p, str(e)))\n\n    for row in iterator:\n        try:\n            img = Image.open(io.BytesIO(row.content)).convert(\"RGB\")\n            np_img = np.array(img) / 255.0\n            batch_shapes.append(np_img.shape[:2])\n            batch_data.append(transform({'image': np_img})['image'][None])\n            batch_paths.append(row.path)\n            \n            if len(batch_data) >= BATCH_SIZE:\n                flush_batch()\n                batch_data, batch_paths, batch_shapes = [], [], []\n        except: pass\n        \n    flush_batch()\n    return iter(results)","metadata":{"execution":{"iopub.status.busy":"2025-12-28T17:10:24.543538Z","iopub.execute_input":"2025-12-28T17:10:24.543788Z","iopub.status.idle":"2025-12-28T17:10:24.55835Z","shell.execute_reply.started":"2025-12-28T17:10:24.54377Z","shell.execute_reply":"2025-12-28T17:10:24.557805Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**MapReduce**","metadata":{}},{"cell_type":"code","source":"def run_job():    \n    # Tắt session cũ để giải phóng GPU\n    try: SparkSession.getActiveSession().stop()\n    except: pass\n\n    # Khởi tạo Spark\n    spark = SparkSession.builder \\\n        .master(\"local[*]\") \\\n        .appName(\"Depth_Final_Stable\") \\\n        .config(\"spark.driver.memory\", \"14g\") \\\n        .config(\"spark.executor.memory\", \"14g\") \\\n        .getOrCreate()\n\n    print(\">> Đang đọc dữ liệu...\")\n    CONFIG_INPUT = \"/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train/*\"\n    \n    df = spark.read.format(\"binaryFile\") \\\n        .option(\"recursiveFileLookup\", \"true\") \\\n        .load(CONFIG_INPUT) \\\n        .filter(col(\"path\").rlike(\"(?i).*\\\\.jpg$\"))\n\n    # Dùng 40 partitions để cân bằng tải\n    NUM_PARTITIONS = 40\n    print(f\">> ⚡ REPARTITION: {NUM_PARTITIONS} Tasks\")\n    \n    count = df.repartition(NUM_PARTITIONS).rdd.mapPartitions(process_partition).count()\n    print(f\">> ✅ XONG! Đã xử lý: {count} ảnh.\")\n    spark.stop()\n\nif __name__ == \"__main__\":\n    run_job()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T17:10:24.559406Z","iopub.execute_input":"2025-12-28T17:10:24.559631Z","iopub.status.idle":"2025-12-28T17:24:42.839872Z","shell.execute_reply.started":"2025-12-28T17:10:24.559606Z","shell.execute_reply":"2025-12-28T17:24:42.836579Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/12/28 17:10:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"name":"stdout","text":">> Đang đọc dữ liệu...\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":">> ⚡ REPARTITION: 40 Tasks\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n  warnings.warn(\"xFormers is not available (SwiGLU)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n  warnings.warn(\"xFormers is not available (Attention)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n  warnings.warn(\"xFormers is not available (Block)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n  warnings.warn(\"xFormers is not available (SwiGLU)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n  warnings.warn(\"xFormers is not available (Attention)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n  warnings.warn(\"xFormers is not available (Block)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n  warnings.warn(\"xFormers is not available (SwiGLU)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n  warnings.warn(\"xFormers is not available (Attention)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n  warnings.warn(\"xFormers is not available (Block)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n  warnings.warn(\"xFormers is not available (SwiGLU)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n  warnings.warn(\"xFormers is not available (Attention)\")\n/kaggle/working/torchhub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n  warnings.warn(\"xFormers is not available (Block)\")\nERROR:root:Exception while sending command.                        (4 + 4) / 40]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 511, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n                          ^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: reentrant call inside <_io.BufferedReader name=53>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\nERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 511, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/context.py\", line 381, in signal_handler\n    self.cancelAllJobs()\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n    self._jsc.sc().cancelAllJobs()\n    ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n    raise Py4JError(\npy4j.protocol.Py4JError: An error occurred while calling o18.sc\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/501294556.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mrun_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_47/501294556.py\u001b[0m in \u001b[0;36mrun_job\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\">> ⚡ REPARTITION: {NUM_PARTITIONS} Tasks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_PARTITIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_partition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\">> ✅ XONG! Đã xử lý: {count} ảnh.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2314\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \"\"\"\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[NumberOrArray]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2289\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \"\"\"\n\u001b[0;32m-> 2291\u001b[0;31m         return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   2292\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n","\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe"],"ename":"Py4JError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"**Result**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport glob\nfrom PIL import Image\nimport os\n\n# 1. Cấu hình đường dẫn\noutput_dir = \"/kaggle/working/depth_output\"\n# Đường dẫn folder chứa ảnh gốc (Dựa trên config input ở cell trên)\ninput_dir = \"/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train\"\n\n# 2. Lấy danh sách ảnh kết quả\noutput_paths = sorted(glob.glob(os.path.join(output_dir, \"*.png\")))\n\nif not output_paths:\n    print(\"❌ Không tìm thấy ảnh nào trong folder output!\")\nelse:\n    print(f\"✅ Tìm thấy {len(output_paths)} ảnh kết quả.\")\n    \n    # Số lượng ảnh cần show (max 5)\n    num_show = min(5, len(output_paths))\n    \n    # Tạo khung hình: 2 hàng (Input - Output) x num_show cột\n    fig, axes = plt.subplots(2, num_show, figsize=(4 * num_show, 7))\n    \n    # Xử lý trường hợp chỉ có 1 ảnh (axes không phải mảng 2 chiều)\n    if num_show == 1:\n        axes = axes.reshape(2, 1)\n\n    for i in range(num_show):\n        # --- XỬ LÝ ẢNH OUTPUT (DEPTH) ---\n        out_path = output_paths[i]\n        out_img = Image.open(out_path)\n        filename = os.path.basename(out_path) # vd: hinh_anh.png\n        basename = os.path.splitext(filename)[0] # vd: hinh_anh\n        \n        # --- TÌM ẢNH INPUT TƯƠNG ỨNG ---\n        # Thử tìm file .jpg trước, nếu không có thì tìm .png\n        in_path = os.path.join(input_dir, basename + \".jpg\")\n        if not os.path.exists(in_path):\n            in_path = os.path.join(input_dir, basename + \".png\")\n            \n        # --- HIỂN THỊ ---\n        # 1. Hàng trên: Ảnh gốc (Input)\n        if os.path.exists(in_path):\n            in_img = Image.open(in_path)\n            axes[0, i].imshow(in_img)\n            axes[0, i].set_title(f\"Input\\n{filename}\", fontsize=9)\n        else:\n            axes[0, i].text(0.5, 0.5, \"Input Not Found\", ha='center')\n            axes[0, i].set_title(\"Missing Input\")\n        axes[0, i].axis('off')\n\n        # 2. Hàng dưới: Ảnh độ sâu (Output)\n        axes[1, i].imshow(out_img, cmap='inferno') # Dùng colormap 'inferno' cho đẹp\n        axes[1, i].set_title(\"Depth Map\", fontsize=9)\n        axes[1, i].axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T17:24:42.840604Z","iopub.status.idle":"2025-12-28T17:24:42.840893Z","shell.execute_reply.started":"2025-12-28T17:24:42.840746Z","shell.execute_reply":"2025-12-28T17:24:42.84076Z"}},"outputs":[{"name":"stderr","text":"[Stage 3:=====>                                                    (4 + 4) / 40]\r","output_type":"stream"}],"execution_count":null}]}